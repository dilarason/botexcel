from llama_cpp import Llama
import os

MODEL_PATH = os.path.expanduser('~/MasaÃ¼stÃ¼/botexcel/models/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf')

llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=1024,
    n_threads=3,
    verbose=False
)

def chat_loop():
    print("ðŸ’¬ BotExcel.AI terminaline hoÅŸ geldin. Ã‡Ä±kmak iÃ§in 'Ã§Ä±k' yaz.\n")
    while True:
        user_input = input("ðŸ‘¤ Sen: ").strip()
        if user_input.lower() in ["Ã§Ä±k", "exit", "quit"]:
            print("ðŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
            break
        prompt = f"KullanÄ±cÄ±: {user_input}\nBot:"
        response = llm(
            prompt,
            max_tokens=128,
            temperature=0.7,
            stop=["\nKullanÄ±cÄ±:"]
        )
        text = response["choices"][0]["text"].strip()
        print(f"ðŸ¤– BotExcel: {text}\n")

if __name__ == "__main__":
    chat_loop()
